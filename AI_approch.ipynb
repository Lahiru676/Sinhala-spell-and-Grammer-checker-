{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AfrdveI5pz_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced Transformer Method,Enhanced LSTM checker and Enhanced Sinhala Text Corrector**"
      ],
      "metadata": {
        "id": "Kr_-CGjkpkoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbjiydz0prZX",
        "outputId": "b47297a0-c423-4a6c-af97-99e454d7f9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: මම පාසල් නිවාඩු කාලයේදී දෙමව්පියන් සමග අනුරාධපුර වන්දනා චාරිකාවක ගියෙය.\n",
            "Corrected Text: <s> <s> <s> <s>මම <s>මමපාසල්නිවාඩුකාලය <s> <s> <s>නිවාඩුකාලය <s>කාලයේදීදෙ <s>දෙමව්\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Input, Conv1D, MaxPooling1D, concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "class EnhancedTransformerChecker:\n",
        "    def __init__(self, model_name='xlm-roberta-base'):\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "            self.model.eval()\n",
        "            # Fine-tune on Sinhala data if available\n",
        "            self.context_window = 5  # Words before and after for context\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Transformer initialization failed: {e}\")\n",
        "            self.tokenizer = None\n",
        "            self.model = None\n",
        "\n",
        "        self.cache = {}  # Cache for frequently corrected words\n",
        "        self.confidence_threshold = 0.75\n",
        "\n",
        "    def build_context(self, words, current_idx):\n",
        "        start = max(0, current_idx - self.context_window)\n",
        "        end = min(len(words), current_idx + self.context_window + 1)\n",
        "        return ' '.join(words[start:end])\n",
        "\n",
        "    def correct_word(self, word, context=''):\n",
        "        if self.model is None:\n",
        "            return word\n",
        "\n",
        "        # Check cache first\n",
        "        if word in self.cache:\n",
        "            return self.cache[word]\n",
        "\n",
        "        try:\n",
        "            # Combine word with context\n",
        "            text_to_check = f\"{context} {word}\" if context else word\n",
        "            inputs = self.tokenizer(text_to_check, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "            # Get word position\n",
        "            word_tokens = self.tokenizer.encode(word, add_special_tokens=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "            predictions = []\n",
        "            for token_idx in range(len(word_tokens)):\n",
        "                logits = outputs.logits[0, token_idx]\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                pred_token = torch.argmax(probs).item()\n",
        "                confidence = probs[pred_token].item()\n",
        "\n",
        "                if confidence > self.confidence_threshold:\n",
        "                    predictions.append(self.tokenizer.decode([pred_token]))\n",
        "                else:\n",
        "                    predictions.append(self.tokenizer.decode([word_tokens[token_idx]]))\n",
        "\n",
        "            corrected = ''.join(predictions).strip()\n",
        "\n",
        "            # Cache the result\n",
        "            self.cache[word] = corrected\n",
        "            return corrected\n",
        "        except Exception as e:\n",
        "            print(f\"Transformer correction failed: {e}\")\n",
        "            return word\n",
        "\n",
        "class EnhancedLSTMChecker:\n",
        "    def __init__(self, max_sequence_length=50):\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.char_tokenizer = Tokenizer(char_level=True, filters='', lower=False)\n",
        "        self.word_tokenizer = Tokenizer(filters='', lower=False)\n",
        "        self.model = None\n",
        "        self.char_patterns = defaultdict(int)\n",
        "        self.word_patterns = defaultdict(int)\n",
        "\n",
        "    def build_advanced_model(self, char_vocab_size, word_vocab_size):\n",
        "        # Character input branch\n",
        "        char_input = Input(shape=(self.max_sequence_length,))\n",
        "        char_emb = Embedding(char_vocab_size, 100)(char_input)\n",
        "\n",
        "        # CNN layers for character patterns\n",
        "        conv1 = Conv1D(64, 3, activation='relu')(char_emb)\n",
        "        pool1 = MaxPooling1D(2)(conv1)\n",
        "        conv2 = Conv1D(128, 3, activation='relu')(pool1)\n",
        "        pool2 = MaxPooling1D(2)(conv2)\n",
        "\n",
        "        # Bidirectional LSTM layers\n",
        "        bilstm1 = Bidirectional(LSTM(128, return_sequences=True))(pool2)\n",
        "        bilstm2 = Bidirectional(LSTM(64))(bilstm1)\n",
        "\n",
        "        # Dense layers with dropout\n",
        "        dense1 = Dense(256, activation='relu')(bilstm2)\n",
        "        dropout1 = Dropout(0.3)(dense1)\n",
        "        dense2 = Dense(128, activation='relu')(dropout1)\n",
        "        dropout2 = Dropout(0.2)(dense2)\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(char_vocab_size, activation='softmax')(dropout2)\n",
        "\n",
        "        model = Model(inputs=char_input, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def prepare_training_data(self, text_data):\n",
        "        if isinstance(text_data, dict):\n",
        "            words = list(text_data.keys())\n",
        "        else:\n",
        "            words = text_data\n",
        "\n",
        "        # Prepare character sequences\n",
        "        char_sequences = []\n",
        "        next_chars = []\n",
        "\n",
        "        for word in words:\n",
        "            for i in range(len(word) - 1):\n",
        "                char_sequences.append(word[i:i+self.max_sequence_length])\n",
        "                next_chars.append(word[i+1])\n",
        "                # Store character patterns\n",
        "                self.char_patterns[word[i:i+3]] += 1\n",
        "\n",
        "        # Fit tokenizers\n",
        "        self.char_tokenizer.fit_on_texts(char_sequences)\n",
        "        self.word_tokenizer.fit_on_texts(words)\n",
        "\n",
        "        # Convert to sequences\n",
        "        X = self.char_tokenizer.texts_to_sequences(char_sequences)\n",
        "        y = self.char_tokenizer.texts_to_sequences(next_chars)\n",
        "\n",
        "        # Pad sequences\n",
        "        X = pad_sequences(X, maxlen=self.max_sequence_length)\n",
        "        y = to_categorical(y, num_classes=len(self.char_tokenizer.word_index) + 1)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, text_data):\n",
        "        try:\n",
        "            X, y = self.prepare_training_data(text_data)\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "            # Build model\n",
        "            self.model = self.build_advanced_model(\n",
        "                len(self.char_tokenizer.word_index) + 1,\n",
        "                len(self.word_tokenizer.word_index) + 1\n",
        "            )\n",
        "\n",
        "            # Callbacks\n",
        "            callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "                ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "            ]\n",
        "\n",
        "            # Train model\n",
        "            self.model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=20,\n",
        "                batch_size=64,\n",
        "                callbacks=callbacks\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LSTM training failed: {e}\")\n",
        "\n",
        "    def correct_word(self, word):\n",
        "        if self.model is None:\n",
        "            return word\n",
        "\n",
        "        try:\n",
        "            # Check character patterns\n",
        "            for i in range(len(word) - 2):\n",
        "                pattern = word[i:i+3]\n",
        "                if self.char_patterns[pattern] == 0:\n",
        "                    # Potential error found, generate correction\n",
        "                    char_seq = self.char_tokenizer.texts_to_sequences([word[i:i+self.max_sequence_length]])\n",
        "                    padded_seq = pad_sequences(char_seq, maxlen=self.max_sequence_length)\n",
        "                    pred = self.model.predict(padded_seq, verbose=0)[0]\n",
        "                    predicted_char = self.char_tokenizer.index_word[np.argmax(pred)]\n",
        "                    word = word[:i+1] + predicted_char + word[i+2:]\n",
        "\n",
        "            return word\n",
        "        except Exception as e:\n",
        "            print(f\"LSTM correction failed: {e}\")\n",
        "            return word\n",
        "\n",
        "class EnhancedSinhalaTextCorrector:\n",
        "    def __init__(self, dictionary_path):\n",
        "        self.dictionary = self.load_dictionary(dictionary_path)\n",
        "        self.transformer_checker = EnhancedTransformerChecker()\n",
        "        self.lstm_checker = EnhancedLSTMChecker()\n",
        "\n",
        "    def load_dictionary(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                return json.load(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dictionary: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def initialize_models(self):\n",
        "        print(\"Training models...\")\n",
        "\n",
        "        sample_incorrect = [\n",
        "            \"මල්\", \"සුගිය\", \"අත්න්ු\", \"කල\", \"ගිය\", \"කරන්\",\n",
        "            \"යන්න\", \"එන්න\", \"බලන්\", \"කියන්\", \"දෙන්න\"\n",
        "        ]\n",
        "\n",
        "        self.lstm_checker.train(self.dictionary)\n",
        "\n",
        "    def correct_text(self, text):\n",
        "        words = text.split()\n",
        "        corrected_words = []\n",
        "\n",
        "        for idx, word in enumerate(words):\n",
        "            context = self.transformer_checker.build_context(words, idx)\n",
        "            corrected_word = self.transformer_checker.correct_word(word, context)\n",
        "            corrected_words.append(corrected_word)\n",
        "\n",
        "        return \" \".join(corrected_words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dictionary_path = \"/content/drive/MyDrive/cleaned_sinhala_words.json\"\n",
        "    corrector = EnhancedSinhalaTextCorrector(dictionary_path)\n",
        "\n",
        "    test_text = \"මම පාසල් නිවාඩු කාලයේදී දෙමව්පියන් සමග අනුරාධපුර වන්දනා චාරිකාවක ගියෙය.\"\n",
        "    print(f\"Original Text: {test_text}\")\n",
        "\n",
        "    corrected_text = corrector.correct_text(test_text)\n",
        "    print(f\"Corrected Text: {corrected_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedSinhalaTextCorrector:\n",
        "    def __init__(self, dictionary_path):\n",
        "        self.dictionary = self.load_dictionary(dictionary_path)\n",
        "        self.transformer_checker = EnhancedTransformerChecker()\n",
        "        self.lstm_checker = EnhancedLSTMChecker()\n",
        "        self.rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "        self.gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "        self.xgb_classifier = xgb.XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "    def load_dictionary(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                return json.load(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dictionary: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def train_ensemble_models(self, correct_words, incorrect_words):\n",
        "        try:\n",
        "            # Combine correct and incorrect words\n",
        "            words = correct_words + incorrect_words\n",
        "            labels = [1] * len(correct_words) + [0] * len(incorrect_words)\n",
        "\n",
        "            # Use TF-IDF for feature extraction\n",
        "            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5))\n",
        "            features = vectorizer.fit_transform(words)\n",
        "\n",
        "            # Train each model\n",
        "            self.rf_classifier.fit(features, labels)\n",
        "            self.gb_classifier.fit(features, labels)\n",
        "            self.xgb_classifier.fit(features, labels)\n",
        "\n",
        "            # Store the vectorizer for future use\n",
        "            self.vectorizer = vectorizer\n",
        "        except Exception as e:\n",
        "            print(f\"Ensemble model training failed: {e}\")\n",
        "\n",
        "    def ensemble_correct_word(self, word):\n",
        "        try:\n",
        "            if not hasattr(self, 'vectorizer'):\n",
        "                return word\n",
        "\n",
        "            # Extract features for the input word\n",
        "            features = self.vectorizer.transform([word])\n",
        "\n",
        "            # Get predictions from each model\n",
        "            rf_pred = self.rf_classifier.predict_proba(features)[0][1]\n",
        "            gb_pred = self.gb_classifier.predict_proba(features)[0][1]\n",
        "            xgb_pred = self.xgb_classifier.predict_proba(features)[0][1]\n",
        "\n",
        "            # Weighted ensemble prediction\n",
        "            ensemble_score = (0.4 * rf_pred + 0.3 * gb_pred + 0.3 * xgb_pred)\n",
        "\n",
        "            # Determine if the word is likely incorrect\n",
        "            if ensemble_score < 0.5:  # Threshold for correction\n",
        "                # Use transformer or LSTM for final correction\n",
        "                return self.lstm_checker.correct_word(word)\n",
        "            return word\n",
        "        except Exception as e:\n",
        "            print(f\"Ensemble correction failed: {e}\")\n",
        "            return word\n",
        "\n",
        "    def correct_text(self, text, use_ensemble=False):\n",
        "        words = text.split()\n",
        "        corrected_words = []\n",
        "\n",
        "        for idx, word in enumerate(words):\n",
        "            if use_ensemble:\n",
        "                corrected_word = self.ensemble_correct_word(word)\n",
        "            else:\n",
        "                context = self.transformer_checker.build_context(words, idx)\n",
        "                corrected_word = self.transformer_checker.correct_word(word, context)\n",
        "            corrected_words.append(corrected_word)\n",
        "\n",
        "        return \" \".join(corrected_words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dictionary_path = \"/content/drive/MyDrive/cleaned_sinhala_words.json\"\n",
        "    corrector = EnhancedSinhalaTextCorrector(dictionary_path)\n",
        "\n",
        "    # Example correct and incorrect words for training\n",
        "    correct_words = [\"මම\", \"පාසල්\", \"නිවාඩු\", \"අනුරාධපුර\"]\n",
        "    incorrect_words = [\"මමම\", \"පසල්\", \"නවාඩු\", \"අනුරාධපුර\"]\n",
        "\n",
        "    # Train ensemble models\n",
        "    corrector.train_ensemble_models(correct_words, incorrect_words)\n",
        "\n",
        "    test_text = \"මමම පසල් නවාඩු කාලයේදී දෙමව්පියන් සමග අනුරාධපුර වන්දනා චාරිකාවක ගියෙය.\"\n",
        "    print(f\"Original Text: {test_text}\")\n",
        "\n",
        "    # Test with transformer-based correction\n",
        "    corrected_text = corrector.correct_text(test_text, use_ensemble=False)\n",
        "    print(f\"Transformer Corrected Text: {corrected_text}\")\n",
        "\n",
        "    # Test with ensemble-based correction\n",
        "    corrected_text_ensemble = corrector.correct_text(test_text, use_ensemble=True)\n",
        "    print(f\"Ensemble Corrected Text: {corrected_text_ensemble}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FEGrJ6mtESF",
        "outputId": "843602bc-f5ef-48c2-ab19-f0051e40f6e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [08:40:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: මමම පසල් නවාඩු කාලයේදී දෙමව්පියන් සමග අනුරාධපුර වන්දනා චාරිකාවක ගියෙය.\n",
            "Transformer Corrected Text: <s>මම <s>ල් <s>නවාම <s>ේදී <s>මමමපසල් <s> <s> <s>නවා <s>කාලයේදීදෙ <s>දෙමව්\n",
            "Ensemble Corrected Text: මමම පසල් නවාඩු කාලයේදී දෙමව්පියන් සමග අනුරාධපුර වන්දනා චාරිකාවක ගියෙය.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset Class for PyTorch\n",
        "class CorrectionDataset(Dataset):\n",
        "    def __init__(self, input_texts, target_texts, tokenizer, max_length=50):\n",
        "        self.input_texts = input_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.input_texts[idx]\n",
        "        target_text = self.target_texts[idx]\n",
        "\n",
        "        # Tokenize the input and target text\n",
        "        input_encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target_encoding = self.tokenizer(\n",
        "            target_text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_encoding[\"input_ids\"].squeeze(),\n",
        "        }\n",
        "\n",
        "# Load Dataset\n",
        "file_path = \"/content/drive/MyDrive/correct incorrect sentences.csv\"  # Update this with your dataset path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare the data\n",
        "data_long = pd.melt(\n",
        "    data,\n",
        "    id_vars=[\"Correct Sentences\"],\n",
        "    value_vars=[\"Incorrect Sentences\", \"Incorrect Sentences.1\"],\n",
        "    var_name=\"Variant Type\",\n",
        "    value_name=\"Incorrect Sentence\"\n",
        ")\n",
        "data_long = data_long.dropna()\n",
        "data_long[\"Correct Sentences\"] = data_long[\"Correct Sentences\"].str.strip()\n",
        "data_long[\"Incorrect Sentence\"] = data_long[\"Incorrect Sentence\"].str.strip()\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(data_long, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizer and Model Initialization\n",
        "model_name = \"t5-small\"  # Replace with \"t5-base\" or \"mbart-large\" for larger models\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = CorrectionDataset(\n",
        "    train_data[\"Incorrect Sentence\"].tolist(),\n",
        "    train_data[\"Correct Sentences\"].tolist(),\n",
        "    tokenizer,\n",
        ")\n",
        "val_dataset = CorrectionDataset(\n",
        "    val_data[\"Incorrect Sentence\"].tolist(),\n",
        "    val_data[\"Correct Sentences\"].tolist(),\n",
        "    tokenizer,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs =20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader)}, Validation Loss = {val_loss / len(val_loader)}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"grammar_correction_model\")\n",
        "tokenizer.save_pretrained(\"grammar_correction_model\")\n",
        "\n",
        "# Prediction Function\n",
        "def predict(input_text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=50, padding=\"max_length\").to(device)\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example Predictions\n",
        "example_sentences = [\n",
        "    \"මම වීදිවල ඇවිදිනවා.\",\n",
        "    \"අපි කඳු නගිනවා.\"\n",
        "]\n",
        "for sentence in example_sentences:\n",
        "    corrected = predict(sentence)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Corrected: {corrected}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txMp6KmieOU4",
        "outputId": "991c69fd-7827-4725-c032-ff69d51cd6f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 1.2462338139512101, Validation Loss = 0.04717972405975865\n",
            "Epoch 2: Train Loss = 0.058796348772486864, Validation Loss = 0.049479847082928304\n",
            "Epoch 3: Train Loss = 0.03860225510232303, Validation Loss = 0.042126755380342086\n",
            "Epoch 4: Train Loss = 0.033208879805645165, Validation Loss = 0.024119753465657274\n",
            "Epoch 5: Train Loss = 0.030135586200167937, Validation Loss = 0.012483481332779892\n",
            "Epoch 6: Train Loss = 0.027230193258776347, Validation Loss = 0.014574354214053\n",
            "Epoch 7: Train Loss = 0.026189704535871137, Validation Loss = 0.01180592596515893\n",
            "Epoch 8: Train Loss = 0.023978185288760126, Validation Loss = 0.01020276329044493\n",
            "Epoch 9: Train Loss = 0.022137888720525162, Validation Loss = 0.012292570911437994\n",
            "Epoch 10: Train Loss = 0.021385067798273295, Validation Loss = 0.011078840489649484\n",
            "Epoch 11: Train Loss = 0.020442507083394696, Validation Loss = 0.01720425397563245\n",
            "Epoch 12: Train Loss = 0.019027026539326322, Validation Loss = 0.011062426094506536\n",
            "Epoch 13: Train Loss = 0.019027543449014122, Validation Loss = 0.008754790465783088\n",
            "Epoch 14: Train Loss = 0.017504882095951815, Validation Loss = 0.011639955834740954\n",
            "Epoch 15: Train Loss = 0.01678695982626202, Validation Loss = 0.04185732688394285\n",
            "Epoch 16: Train Loss = 0.016762344995323492, Validation Loss = 0.007082167665489138\n",
            "Epoch 17: Train Loss = 0.015501745474733868, Validation Loss = 0.008948747564347522\n",
            "Epoch 18: Train Loss = 0.014289159999628152, Validation Loss = 0.007163726672109577\n",
            "Epoch 19: Train Loss = 0.01463297098133789, Validation Loss = 0.01917101760890575\n",
            "Epoch 20: Train Loss = 0.0139514073824548, Validation Loss = 0.008605289955495766\n",
            "Original: මම වීදිවල ඇවිදිනවා.\n",
            "Corrected: .\n",
            "Original: අපි කඳු නගිනවා.\n",
            "Corrected: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Prediction Function\n",
        "def predict(input_text, model, tokenizer, device, max_length=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input sentence\n",
        "        inputs = tokenizer(\n",
        "            input_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate the corrected output\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode the generated sequence\n",
        "        corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return corrected_sentence\n",
        "\n",
        "# Example Predictions\n",
        "example_sentences = [\n",
        "    \"මම වීදිවල ඇවිදිනවා.\",\n",
        "    \"අපි කඳු නගිනවා.\"\n",
        "]\n",
        "\n",
        "# Ensure the model and tokenizer are on the same device\n",
        "model.to(device)\n",
        "\n",
        "# Generate predictions for each example\n",
        "for sentence in example_sentences:\n",
        "    corrected = predict(sentence, model, tokenizer, device)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(f\"Corrected: {corrected}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ato1NlxiT7J",
        "outputId": "e11d9124-18d0-4cbf-c632-ab57fef5b16e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: මම වීදිවල ඇවිදිනවා.\n",
            "Corrected: .\n",
            "Original: අපි කඳු නගිනවා.\n",
            "Corrected: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM based Method"
      ],
      "metadata": {
        "id": "7TKLq-7Qqm1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/grammar_correction_pairs.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_incorrect, test_incorrect, train_correct, test_correct = train_test_split(\n",
        "    data['incorrect_sentence'], data['correct_sentence'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer(oov_token=\"\")\n",
        "tokenizer.fit_on_texts(train_incorrect.tolist() + train_correct.tolist())\n",
        "\n",
        "# Convert text to sequences\n",
        "train_incorrect_sequences = tokenizer.texts_to_sequences(train_incorrect)\n",
        "train_correct_sequences = tokenizer.texts_to_sequences(train_correct)\n",
        "\n",
        "test_incorrect_sequences = tokenizer.texts_to_sequences(test_incorrect)\n",
        "test_correct_sequences = tokenizer.texts_to_sequences(test_correct)\n",
        "\n",
        "# Padding sequences\n",
        "max_len = max(\n",
        "    max(len(seq) for seq in train_incorrect_sequences),\n",
        "    max(len(seq) for seq in train_correct_sequences)\n",
        ")\n",
        "train_incorrect_padded = pad_sequences(train_incorrect_sequences, maxlen=max_len, padding='post')\n",
        "train_correct_padded = pad_sequences(train_correct_sequences, maxlen=max_len, padding='post')[..., None]\n",
        "\n",
        "test_incorrect_padded = pad_sequences(test_incorrect_sequences, maxlen=max_len, padding='post')\n",
        "test_correct_padded = pad_sequences(test_correct_sequences, maxlen=max_len, padding='post')[..., None]\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Build the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(\n",
        "    train_incorrect_padded, train_correct_padded,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "results = lstm_model.evaluate(test_incorrect_padded, test_correct_padded)\n",
        "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
        "\n",
        "# Function to predict corrected sentence\n",
        "def predict_sentence(input_sentence):\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_padded = pad_sequences(input_sequence, maxlen=max_len, padding='post')\n",
        "    predictions = lstm_model.predict(input_padded)\n",
        "    predicted_sequence = tf.argmax(predictions[0], axis=-1).numpy()\n",
        "    predicted_sentence = \" \".join(\n",
        "        [word for word in tokenizer.sequences_to_texts([predicted_sequence])[0].split() if word != \"\"]\n",
        "    )\n",
        "    return predicted_sentence\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"මම ගෙදර යැවෙමු\"\n",
        "predicted_sentence = predict_sentence(input_sentence)\n",
        "print(\"Input Sentence:\", input_sentence)\n",
        "print(\"Predicted Sentence:\", predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o8xvd477jRQ",
        "outputId": "8ce29a32-9b29-4336-faf9-23ba195f9df0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - accuracy: 0.2803 - loss: 3.6158 - val_accuracy: 0.6914 - val_loss: 1.2587\n",
            "Epoch 2/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.7503 - loss: 1.0695 - val_accuracy: 0.8243 - val_loss: 0.7130\n",
            "Epoch 3/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8273 - loss: 0.7026 - val_accuracy: 0.8723 - val_loss: 0.5190\n",
            "Epoch 4/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8842 - loss: 0.5050 - val_accuracy: 0.9141 - val_loss: 0.3747\n",
            "Epoch 5/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9149 - loss: 0.3734 - val_accuracy: 0.9398 - val_loss: 0.2779\n",
            "Epoch 6/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9398 - loss: 0.2756 - val_accuracy: 0.9544 - val_loss: 0.2060\n",
            "Epoch 7/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9544 - loss: 0.2098 - val_accuracy: 0.9685 - val_loss: 0.1535\n",
            "Epoch 8/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9655 - loss: 0.1632 - val_accuracy: 0.9763 - val_loss: 0.1148\n",
            "Epoch 9/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9753 - loss: 0.1232 - val_accuracy: 0.9798 - val_loss: 0.0901\n",
            "Epoch 10/10\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9797 - loss: 0.0980 - val_accuracy: 0.9829 - val_loss: 0.0723\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9822 - loss: 0.0741\n",
            "Test Loss: 0.07397918403148651, Test Accuracy: 0.9824660420417786\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
            "Input Sentence: වාහන පොත බලා ගෙදර වේගයෙන් යැවෙමු\n",
            "Predicted Sentence: වාහන පොත බලා ගෙදර වේගයෙන් යැවේ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "input_sentence = \"මම ගෙදර යැවෙමු\"\n",
        "predicted_sentence = predict_sentence(input_sentence)\n",
        "print(\"Input Sentence:\", input_sentence)\n",
        "print(\"Predicted Sentence:\", predicted_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl1gJmLU8y56",
        "outputId": "fb8db23a-3f9f-4f6b-9f32-a34a840b0612"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "Input Sentence: මම ගෙදර යැවෙමු\n",
            "Predicted Sentence: මම ගෙදර යැවෙමි\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain huggingface-hub sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBP2AoFYdW4H",
        "outputId": "2961eed2-2b05-4f11-d04c-0e961fafbaa6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-iN5DdoeAEa",
        "outputId": "5e992a13-0f33-4c46-eb84-327c7695f847"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.29 (from langchain)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed langchain-0.3.14 langchain-core-0.3.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uI5x3jyeWTt",
        "outputId": "f06c5962-2dfd-4fc2-dfa7-57f9c1d418fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.14 marshmallow-3.23.3 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLbgJo_He0--",
        "outputId": "bac1e2b8-074d-4b29-9be8-7982a4b251e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Fetch the API key securely\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Use the fetched API key in your application\n",
        "checker = GrammarChecker(api_key=api_key)\n"
      ],
      "metadata": {
        "id": "KKa2rhjbGLdQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-based Grammar and spell Checker with RAG Architecture"
      ],
      "metadata": {
        "id": "QxRLD5BlrBPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "class GrammarChecker:\n",
        "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", api_key: str = None):\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "        )\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=model_name,\n",
        "            temperature=0,\n",
        "            api_key=api_key\n",
        "        )\n",
        "        self.vector_store = None\n",
        "        self.prompt = PromptTemplate(\n",
        "            template=\"\"\"Context: {context}\n",
        "\n",
        "Input sentence: {input_text}\n",
        "\n",
        "Analyze the given Sinhala sentence and provide:\n",
        "1. List of grammatical or spelling errors\n",
        "2. Explanations for each error\n",
        "3. Corrected sentence\n",
        "\n",
        "Response in format:\n",
        "Errors:\n",
        "[List errors]\n",
        "\n",
        "Explanations:\n",
        "[Explain each error]\n",
        "\n",
        "Corrected:\n",
        "[Corrected sentence]\"\"\",\n",
        "            input_variables=[\"context\", \"input_text\"]\n",
        "        )\n",
        "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
        "\n",
        "    def load_training_data(self, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        documents = []\n",
        "        for item in data['training_data']:\n",
        "            content = f\"\"\"\n",
        "            Incorrect: {item['incorrect_sentence']}\n",
        "            Correct: {item['correct_sentence']}\n",
        "            Error: {item['error_details']}\n",
        "            \"\"\"\n",
        "            documents.append(Document(page_content=content))\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        split_docs = text_splitter.split_documents(documents)\n",
        "        self.vector_store = FAISS.from_documents(split_docs, self.embeddings)\n",
        "\n",
        "    def check_grammar(self, text: str, k: int = 3) -> str:\n",
        "        if not self.vector_store:\n",
        "            raise ValueError(\"Training data not loaded. Call load_training_data first.\")\n",
        "\n",
        "        similar_examples = self.vector_store.similarity_search(text, k=k)\n",
        "        context = \"\\n\".join([doc.page_content for doc in similar_examples])\n",
        "\n",
        "        response = self.chain.run(\n",
        "            context=context,\n",
        "            input_text=text\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    def batch_check(self, texts: List[str]) -> List[str]:\n",
        "        return [self.check_grammar(text) for text in texts]\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    # Load training data\n",
        "    checker.load_training_data(\"/content/drive/MyDrive/sinhala_grammar_training.json\")\n",
        "\n",
        "    # Example usage\n",
        "    text = \"අපි උත්සාහයෙන් වැඩ කරමි\"\n",
        "    result = checker.check_grammar(text)\n",
        "    print(f\"Input: {text}\\n\")\n",
        "    print(f\"Analysis:\\n{result}\")\n",
        "\n",
        "    # Batch processing example\n",
        "    texts = [\n",
        "        \"මම කෑම ගනිමු\",\n",
        "        \"අපි පොත කියවමි\",\n",
        "        \"මේ දින වල සිග්‍රයන් පැතිර යන වසන්ගත උන රෝගයක් නිසා අප විශ්විද්‍යායේ බොහෝ සිසුන් පීඩාවට පත්වී සිටී.\"\n",
        "    ]\n",
        "    results = checker.batch_check(texts)\n",
        "    for text, result in zip(texts, results):\n",
        "        print(f\"\\nInput: {text}\")\n",
        "        print(f\"Analysis:\\n{result}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ1DdAjM94H1",
        "outputId": "39e19e8a-db83-48e4-d69f-efc595a64497"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: අපි උත්සාහයෙන් වැඩ කරමි\n",
            "\n",
            "Analysis:\n",
            "Errors:\n",
            "1. වැඩ කරමි\n",
            "\n",
            "Explanations:\n",
            "1. Verb 'කරමි' does not match the subject 'අපි'. It should end with 'මු'.\n",
            "\n",
            "Corrected:\n",
            "අපි උත්සාහයෙන් වැඩ කරමු\n",
            "\n",
            "Input: මම කෑම ගනිමු\n",
            "Analysis:\n",
            "Errors:\n",
            "1. Verb Agreement Error\n",
            "\n",
            "Explanations:\n",
            "1. The verb 'ගනිමු' does not match the subject 'මම'. It should end with 'මි'.\n",
            "\n",
            "Corrected:\n",
            "මම කෑම ගනිමි\n",
            "\n",
            "Input: අපි පොත කියවමි\n",
            "Analysis:\n",
            "Errors:\n",
            "1. කියවමි should be කියවිමි\n",
            "\n",
            "Explanations:\n",
            "1. Verb 'කියවමි' does not match the subject 'අපි'. It should end with 'කියවිමි'.\n",
            "\n",
            "Corrected:\n",
            "අපි පොත කියවිමි\n",
            "\n",
            "Input: මේ දින වල සිග්‍රයන් පැතිර යන වසන්ගත උන රෝගයක් නිසා අප විශ්විද්‍යායේ බොහෝ සිසුන් පීඩාවට පත්වී සිටී.\n",
            "Analysis:\n",
            "Errors:\n",
            "1. වසන්ගත should be වසන්ගත්\n",
            "2. උන should be උනා\n",
            "3. පීඩාවට should be පීඩාවෙන්\n",
            "\n",
            "Explanations:\n",
            "1. The word වසන්ගත should have the case marker වසන්ගත් to indicate the direct object.\n",
            "2. The verb උන should be conjugated as උනා to match the subject.\n",
            "3. The preposition පීඩාවට should be followed by the case marker පීඩාවෙන් to indicate the direction.\n",
            "\n",
            "Corrected:\n",
            "මේ දින වල සිග්‍රයන් පැතිර යන වසන්ගත් උනා රෝගයක් නිසා අප විශ්විද්‍යායේ බොහෝ සිසුන් පීඩාවෙන් පත්වී සිටී.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best one is GPT based Grammar and spell checker"
      ],
      "metadata": {
        "id": "igUlC20KBj9N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aMJbZgqXhnE6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}